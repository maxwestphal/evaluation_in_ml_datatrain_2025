{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731344a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9675c929",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "To know the basics regarding...\n",
    "\n",
    "-   our example datasets/tasks,\n",
    "-   machine learning in R (with the **mlr3** package),\n",
    "-   selected learning algorithms for this course,\n",
    "-   ML task definition.\n",
    "\n",
    "## 1.1 Introduction / Reading (30 min)\n",
    "\n",
    "### Data\n",
    "\n",
    "We prepared four ML tasks based on two datasets for this course:\n",
    "\n",
    "(a) **ctg** (simplified) $\\rightarrow$ a binary classification task (target: \"status\")\n",
    "(b) **ctg3** (original) $\\rightarrow$ a 3-class classification task (target: \"status\")\n",
    "(c) **support_regr** $\\rightarrow$ a regression task (target: \"totcst\" - total costs)\n",
    "(d) **support_surv** $\\rightarrow$ a survival task (target: \"death\" & \"d.time\")\n",
    "\n",
    "Please read the basic documentation on the two datasets which is provided here\n",
    "\n",
    "-   ctg:\n",
    "    -   <https://archive.ics.uci.edu/ml/datasets/cardiotocography>\n",
    "-   support:\n",
    "    -   <https://pubmed.ncbi.nlm.nih.gov/7810938/> $\\rightarrow$ Abstract\n",
    "    -   <https://hbiostat.org/data/repo/supportdesc> $\\rightarrow$ \"support2 Dataset\"\n",
    "\n",
    "Throughout the course, all assignments will initially focus on binary classification tasks and hence the **ctg** (simplified) dataset. For these tasks a sample solution will be provided. If you are a beginner, we recommend to stick to this dataset!\n",
    "\n",
    "If you have additional time or bring some prior knowledge, we encourage you to consider translating the assignments to other datasets. Of course, you are also allowed to work on datasets from your own work!\n",
    "\n",
    "### Machine learning in R (with mlr3)\n",
    "\n",
    "In R, several frameworks are available for model development (mlr3, caret, tidymodels). In this course, mlr3-based sample solutions will be provided. However, you can utilize any other framework (or even another language) if you prefer.\n",
    "\n",
    "If you indeed use **mlr3** for the exercises, we recommend the **mlr3 book** at <https://mlr3book.mlr-org.com/index.html> as a useful resource, in particular the first two sections \"Introduction and Overview\" and \"Basics\". You can open a separate R script (ctrl-shift-n) and experiment with the code from the book yourself.\n",
    "\n",
    "We encourage you to revisit the mlr3 book whenever you need help during the course. In addition, a wide variety of code examples is provided in the **mlr3 gallery** at <https://mlr3gallery.mlr-org.com/>.\n",
    "\n",
    "### Machine learning in Python\n",
    "\n",
    "In python, there also exist several packages for model developement and testing. For simple machine learning and relaterd functions, we recommend the package scikit-learn, which contains many of the most commonly used models and utility functions.\n",
    "\n",
    "For anything table/dataframe related, pandas is the most commonly used package in python.\n",
    "\n",
    "numpy is a package surrounding matrices (potentially in high dimensions) called arrays. These are often used as the internal data structure in machine learning models as they are easier to include in computations than dataframes.\n",
    "\n",
    "matplotlib and seaborn are two packages for creating illustrations. Seaborn plots usually look nicer out-of-the-box, whereas matplotlib (which seaborn is based on) is more barebones but its plots are more easily customizable\n",
    "\n",
    "scipy is a package that we will use mainly in Ex.3 for an easier time dealing with probability distributions; for example for evaluating density functions of common distributions.  \n",
    "\n",
    "### ML algorithms\n",
    "\n",
    "While this is not a course on the fundamentals of machine learning, you may want to quickly refresh your knowledge on two basic ML algorithms for tabular data which we will utilize later:\n",
    "\n",
    "-   **Elastic Net** (*glmnet*): <https://en.wikipedia.org/wiki/Elastic_net_regularization>\n",
    "-   **Random Forests** (*ranger*): <https://en.wikipedia.org/wiki/Random_forest>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643a8f9",
   "metadata": {},
   "source": [
    "## 1.2 Data loading and exploration (20 mins)\n",
    "\n",
    "### Task\n",
    "\n",
    "For one (e.g. the **ctg** dataset) or multiple of the datasets mentioned above, load the data (e.g. with the pyreadr package)\n",
    "and explore it data via common pandas functions such as\n",
    "\n",
    "-   `shape()`\n",
    "-   `columns()`\n",
    "-   `head()`\n",
    "\n",
    "You could also check for NA entries, or create explorative (scatter-) plots with matplotlib or seaborn\n",
    "A helpful reference can be found under <https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf>\n",
    "\n",
    "Additionally, create a (simple) plot to visualize the target variable (\"status\" for the **ctg** dataset/task) to check if the data is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b98483",
   "metadata": {},
   "source": [
    "## 1.3 ML task definition (10 minutes)\n",
    "\n",
    "### Task\n",
    "\n",
    "Based on the variable names and/or documentation, should any of the feature variables not be used for training?\n",
    "\n",
    "How can we describe the ML task now in terms of\n",
    "\n",
    "-   task type\n",
    "-   outcome\n",
    "-   features\n",
    "\n",
    "To my (Tom's) knowledge, no equivalent to the `as_task_classif` function of **mlr3** exists in python - at least not in sklearn.\n",
    "\n",
    "Usually, one just manually implements the corresponding steps ad hoc.\n",
    "\n",
    "First, you should separate your data into a target array (y) and a feature matrix (X), for example with the `pop()` function\n",
    "\n",
    "Make sure that the data is coded in a way that the algorithm you're using later on can handle it and that irrelevant columns are dropped. For example it might make sense to recode binary variables as 0/1. Hint: `replace()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9319fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a664b2",
   "metadata": {},
   "source": [
    "## 1.4 ML algorithm investigation (10 minutes)\n",
    "\n",
    "### Task\n",
    "\n",
    "Identify suitable learning algorithms for the **ctg** task (or your task of interest) in **sklearn** (or your framework of choice) and study the implementation and the documentation (for example on <https://scikit-learn.org/stable/api/index.html>).\n",
    "\n",
    "Usually, it is especially important to have a look at the Parameters needed for initialization, the fit function and the predict function\n",
    "\n",
    "If necessary, download additional required packages for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b13423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171f992",
   "metadata": {},
   "source": [
    "## 1.5 First model training (20 minutes)\n",
    "\n",
    "### Task\n",
    "\n",
    "For the **ctg** dataset/task, train a simple elastic net model and estimate the AUC on an independent test set. Only train the model with the hyperparameter $\\lambda = 0.01$. For this purpose, use 70% of observations (rows) for training and 30% for testing.\n",
    "\n",
    "Create a evaluation data.frame, i.e. a matrix with two columns: the true labels and the model predictions for the test data. How could we summarize this matrix further?\n",
    "\n",
    "Hints: \n",
    "- sklearn contains a function for train/test splitting\n",
    "- sklearn contains several functions for a wide range of model metrics. ROC-AUC included\n",
    "- there is also a function for the confusion matrix in sklearn \n",
    "- a seaborn heatmap could be a nice vizualization for the confusion matrix you just obtained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea208817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731344a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "import pyreadr\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9675c929",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "Be able to...\n",
    "\n",
    "- define and utilize different data splitting techniques,\n",
    "- compare and interpret evaluation results depending on the experimental designs.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "From previous exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "data_ctg = pyreadr.read_r(os.path.join(os.getcwd(), \"data_ctg.rds\"))[None]\n",
    "\n",
    "# split into X and y:\n",
    "y = data_ctg.pop(\"status\")\n",
    "X = data_ctg\n",
    "\n",
    "#change \"Year\" to integer (otherwise issues arise with Logistic Regression):\n",
    "X[\"Year\"] = X[\"Year\"].astype(int)\n",
    "# change  y to a 0/1 encoding; the models would probably work with the current encoding, but this is more controlleable:\n",
    "y = y.replace({\"normal\": \"0\", \"suspect\": \"1\"})\n",
    "y = y.astype(int)\n",
    "\n",
    "# if you think further data preprocessing is necessary, feel free to add it here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d75b129",
   "metadata": {},
   "source": [
    "## Setup: algorithms \\& hyperparamers \n",
    "\n",
    "This section shows a very basic implemkentation of hyperparameter searches. You will likely have to re-use so of this code later for your custom cross-validation strategies. \n",
    "\n",
    "Unlike in mlr3, there is (to my knowledge) no package that defines recommended hyperparameter search spaces for you in python.\n",
    "You usually have to come up with sensible values yourself and define a space and search algorithm. As we aren't **that** concerned with cutting-edge performance today, I used filled  in some values that seemed plauisible to me (Tom).\n",
    "\n",
    "For a pointer what these \"sensible\" values might usually be, you can of course always reference the values that mlr3 uses (the \"vals\" give an upper and lower bound for each hyperparameter):\n",
    "<https://github.com/mlr-org/mlr3tuningspaces/blob/main/R/tuning_spaces_default.R>\n",
    "Keep in mind that the hyperparameters might be named differently or that there might be subtle differences in implementations. For example, sklearn's LogisticRegression expects a parameter C which is the **inverse** of the regularization strength.\n",
    "\n",
    "You could then, for each hyperparameter, define multiple values in between those bounds, e.g. np.linspace() or range() and use a grid search to find the best value from the grid:\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV>\n",
    "Note: it is probably a good idea to run multiple runs in parallel using the `n_jobs` parameter.\n",
    "\n",
    "The GridSearchCV object can be initialized with a custom `cv` parameter. This determines the cross-validation strategy. The default is `cv=5`, meaning the algorithm will do a 5-fold split to determine the best model on average.\n",
    "We will later ajust this for custom strategies and just leave it as is for now (or set it to `cv=2` to reduce the computational load).\n",
    "\n",
    "You could (maybe not today) also use more sophisticated search algorithms that maybe find a better/faster solution:\n",
    "<https://scikit-learn.org/stable/modules/grid_search.html>\n",
    "\n",
    "You might also want to give the metric you're using to measure model performance some thought. We will use the ROC-AUC which is a standard metric for binary classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9319fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the metric we will use to compare model performance:\n",
    "scorer = make_scorer(roc_auc_score, response_method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bc89480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many hyperparameters should we sample randomly per algorithm? Notice that for a grid search, this is not as straightforward as in mlr3 since we don't have a function that magically fills the search space with a given number of samples.\n",
    "# We have to manually define how many values we want to try for each hyperparameter.\n",
    "# for simplicity, we will just use two different values depending on how many hyperparameters we have to tune; usually we would probably prefer these to be larger:\n",
    "n_hyp_small = 10\n",
    "n_hyp_large = 50\n",
    "\n",
    "# Define the model for which we search hyperparameters; GridSearch expects an initialized model object and will then set the hyperparameters on a model copy internally.\n",
    "elastic_net_classifier = LogisticRegression(penalty='elasticnet', solver = \"saga\", l1_ratio=0.5, max_iter=10000)\n",
    "\n",
    "# define a grid of hyperparameters to search over:\n",
    "glmnet_hyperparam_grid = {\"C\": np.linspace(0.01, 100, n_hyp_large)}\n",
    "\n",
    "# define the gridsearch object:\n",
    "ex_glmnet_hyperparam_gridsearch = GridSearchCV(elastic_net_classifier, glmnet_hyperparam_grid, scoring=scorer, cv=2, n_jobs=-1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18617b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Best hyperparameters for glmnet: {'C': 57.14714285714285}\n"
     ]
    }
   ],
   "source": [
    "# optional: run the gridsearch; we will later use custom cross-validation strategies, so this data will not be used again:\n",
    "ex_glmnet_hyperparam_gridsearch.fit(X, y)\n",
    "print(\"Best hyperparameters for glmnet:\", ex_glmnet_hyperparam_gridsearch.best_params_)\n",
    "# Note that one could also have used LogisticRegressionCV with an inbuilt cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b13423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing for a random forest classifier:\n",
    "rf_classifier = RandomForestClassifier(bootstrap=True)\n",
    "\n",
    "rf_hyperparam_grid = {\n",
    "    \"n_estimators\": np.linspace(1, 2000, n_hyp_small, dtype=int),           # corresponds to num.trees in mlr3\n",
    "    \"max_features\": np.linspace(0.1, 1.0, n_hyp_small//2),       # corresponds to mtry.ratio (fraction of features consiodered at each split) in mlr3\n",
    "    \"max_samples\": np.linspace(0.1, 1.0, n_hyp_small//2)         # corresponds to sample.fraction (fraction of samples used for each tree) in mlr3\n",
    "}\n",
    "ex_rf_hyperparam_gridsearch = GridSearchCV(rf_classifier, rf_hyperparam_grid, scoring=scorer, cv=2, n_jobs=-1, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62e5f7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 250 candidates, totalling 500 fits\n",
      "Best hyperparameters for random forest: {'max_features': 0.1, 'max_samples': 0.55, 'n_estimators': 889}\n"
     ]
    }
   ],
   "source": [
    "# optional: run the gridsearch; we will later use custom cross-validation strategies, so this data will not be used again:\n",
    "ex_rf_hyperparam_gridsearch.fit(X, y)\n",
    "print(\"Best hyperparameters for random forest:\", ex_rf_hyperparam_gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beaecbe",
   "metadata": {},
   "source": [
    "## 2.1 Train-tune-test split (20 min)\n",
    "\n",
    "### Task\n",
    "\n",
    "Derive a random train-tune-test (60-20-20) split by explicitly setting three sets of indices (row_ids). Hereby, make sure to set a random seed (we'll use 42) and to stratify the splits for the outcome variable.\n",
    "\n",
    "- `train_indices = ...`\n",
    "- `tune_indices = ...`\n",
    "- `test_indices = ...`\n",
    "\n",
    "Optional: do a GridSearch on the obtained split\n",
    "\n",
    "Even more Optional: test the best model on the test data set; how would you go about refitting the model on the combined train/tune data? There is a simple method; maybe have a look at the `GridSearchCV` documentation again.\n",
    "\n",
    "### Very important hint: If you want to use your train/validation splits in the Grid-search, the easiest method I found is to pass a list of length 1 of tuples [(train_indices, tune_indices)]\n",
    "\n",
    "Hint: \n",
    "- sklearn includes a function `train_test_split` which I would advise to use. However, there is no immediate 3-way split function\n",
    "- `train_test_split` does not output indices; but if you input an array of indices of the right shape, you can get the right effect\n",
    "- seeds in sklearn are set in each function; so for example train_test_split(..., random_state = 42)\n",
    "- depending on the dataset, shuffling the data might be important\n",
    "- check that your split worked as intended afterwards ( for ex. you could check the sizes of each dataset or the fractions of positive samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea208817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed\n",
    "# get train indices:\n",
    "train_tune_indices, test_indices = train_test_split(np.arange(X.shape[0]), test_size=0.2, random_state=42, shuffle=True, stratify=y)\n",
    "train_indices, tune_indices = train_test_split(train_tune_indices, test_size=0.25, random_state=42, shuffle=True, stratify=y.iloc[train_tune_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce3a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 50 candidates, totalling 50 fits\n",
      "Best hyperparameters for glmnet: {'C': 2.050612244897959}\n",
      "best score on tune data: 0.9510257544313273\n",
      "score on test data: 0.962099819684959\n"
     ]
    }
   ],
   "source": [
    "glmnet_hyperparam_gridsearch = GridSearchCV(elastic_net_classifier, glmnet_hyperparam_grid, scoring=scorer, cv = [(train_indices, tune_indices)], n_jobs=-1, verbose = 1, refit=True)\n",
    "glmnet_hyperparam_gridsearch.fit(X, y)\n",
    "print(\"Best hyperparameters for glmnet:\", glmnet_hyperparam_gridsearch.best_params_)\n",
    "X_test, y_test = X.iloc[test_indices], y.iloc[test_indices]\n",
    "print(f\"score on test data: {scorer(glmnet_hyperparam_gridsearch, X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9107a",
   "metadata": {},
   "source": [
    "## 2.2 Nested cross-validation\n",
    "\n",
    "### Task\n",
    "\n",
    "Define a nested CV design with 5 folds for the outer loop and a simple holdout design for the inner loop (75% training, 25% tuning).\n",
    "\n",
    "I think there is two main ways of approaching this. \n",
    "\n",
    "The first, which I used in my solution, is to make use of the `cross_validate` function. For reference, see \n",
    "<https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html>\n",
    "Essentially, you put the GridSearchCV as an estimator into the cross_validate.\n",
    "The difficult part is, that there seems to be no easy way of telling the inner Gridsearch to just use a single split; a true nested cv would be much easier to implement :D\n",
    "\n",
    "In my solution, I therefore implement a small `train_test_splitter()` class that implements a `split(self, X, y=None, groups=None)` function returning `[(train_indices, test_indices)]` and a `get_n_splits(self, *args, **kwargs)` function always returning 1. See also <https://scikit-learn.org/stable/glossary.html#term-CV-splitter>\n",
    "\n",
    "As an alternative method, you could use the function KFold and essentially implement the loop manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d071dca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 50 candidates, totalling 50 fits\n",
      "1656\n",
      "1242\n",
      "Fitting 1 folds for each of 50 candidates, totalling 50 fits\n",
      "1656\n",
      "1242\n",
      "Fitting 1 folds for each of 50 candidates, totalling 50 fits\n",
      "1656\n",
      "1242\n",
      "Fitting 1 folds for each of 50 candidates, totalling 50 fits\n",
      "1656\n",
      "1242\n",
      "Fitting 1 folds for each of 50 candidates, totalling 50 fits\n",
      "1656\n",
      "1242\n"
     ]
    }
   ],
   "source": [
    "class train_test_splitter():\n",
    "    def __init__(self, train_size=0.75, random_state=42, shuffle=True):\n",
    "        self.train_size = train_size\n",
    "        self.random_state = random_state\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = X.shape[0]\n",
    "        print(n_samples)\n",
    "        indices = np.arange(n_samples)\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(indices)\n",
    "        train_size = int(self.train_size * n_samples)\n",
    "        print(train_size)\n",
    "        train_indices = indices[:train_size]\n",
    "        test_indices = indices[train_size:]\n",
    "        return [(train_indices, test_indices)]\n",
    "\n",
    "    def get_n_splits(self, *args, **kwargs):\n",
    "        return 1\n",
    "    \n",
    "custom_cv = train_test_splitter(train_size=0.75, random_state=42)\n",
    "\n",
    "glmnet_hyperparam_gridsearch = GridSearchCV(elastic_net_classifier, glmnet_hyperparam_grid, scoring=scorer, cv = custom_cv, n_jobs=-1, verbose = 1)\n",
    "ncv_results = cross_validate(glmnet_hyperparam_gridsearch, X, y, cv=5, scoring=scorer, return_train_score=True, return_estimator=True, return_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e7445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time   param_C  \\\n",
      "0       1.781557           0.0         0.002167             0.0  0.010000   \n",
      "1       2.250076           0.0         0.002274             0.0  2.050612   \n",
      "2       2.290004           0.0         0.002225             0.0  4.091224   \n",
      "3       2.265292           0.0         0.003011             0.0  6.131837   \n",
      "4       2.309574           0.0         0.002222             0.0  8.172449   \n",
      "\n",
      "                     params  split0_test_score  mean_test_score  \\\n",
      "0               {'C': 0.01}           0.922340         0.922340   \n",
      "1  {'C': 2.050612244897959}           0.932291         0.932291   \n",
      "2  {'C': 4.091224489795918}           0.932291         0.932291   \n",
      "3  {'C': 6.131836734693877}           0.932291         0.932291   \n",
      "4  {'C': 8.172448979591836}           0.932291         0.932291   \n",
      "\n",
      "   std_test_score  rank_test_score  fold  \n",
      "0             0.0               50     1  \n",
      "1             0.0               36     1  \n",
      "2             0.0               36     1  \n",
      "3             0.0               36     1  \n",
      "4             0.0               36     1  \n"
     ]
    }
   ],
   "source": [
    "# we join the results of all outer folds together:\n",
    "all_results = pd.concat([pd.DataFrame(ncv_results[\"estimator\"][i].cv_results_).assign(fold=i+1) for i in range(len(ncv_results[\"estimator\"]))], ignore_index=True)\n",
    "print(all_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d1e7e",
   "metadata": {},
   "source": [
    "## 2.3 Temporal data split\n",
    "\n",
    "### Task\n",
    "\n",
    "Similar to tasks 2.1 and 2.2, create a temporal data split. For this, we want to create several splits where each time we use data from \n",
    "one year for training and data from the next year for testing.\n",
    "\n",
    "- What is different for this splitting strategy compared to cross-validation?\n",
    "- Are there other temporal splits that may be reasonable here? \n",
    "\n",
    "Hints:\n",
    "- for implementation, I would suggest a manual approach similar to Ex. 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ea42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf80e9",
   "metadata": {},
   "source": [
    "### Solution: \n",
    "- Internal-external validation (accessing temporal transferability) instead of internal validation.\n",
    "In addition, not all subsets of the data have equal size.\n",
    "- E.g. train in 2 years, test in the next year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f715b3",
   "metadata": {},
   "source": [
    "## 2.4 Model training and selection (for completeness only, no tasks here)\n",
    "\n",
    "After hyperparameter tuning, we need to select models for testing/evaluation. \n",
    "\n",
    "We implemented some of the model tuning and selection in the above exercises, but to ensure that there is a consistent starting point for this exercise, we will rely on data obtained from the `\"scripts/2_rwd_ctg_model_dev\"` script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f49b83ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_ids   truth response_glmnet  prob.suspect_glmnet  prob.normal_glmnet  \\\n",
      "0        3  normal          normal             0.026135            0.973865   \n",
      "1       19  normal          normal             0.006974            0.993026   \n",
      "2       28  normal          normal             0.000184            0.999816   \n",
      "3       31  normal          normal             0.000017            0.999983   \n",
      "4       47  normal          normal             0.000139            0.999861   \n",
      "\n",
      "  response_ranger  prob.suspect_ranger  prob.normal_ranger  \n",
      "0          normal             0.442594            0.557406  \n",
      "1          normal             0.001667            0.998333  \n",
      "2          normal             0.027838            0.972162  \n",
      "3          normal             0.043218            0.956782  \n",
      "4          normal             0.001630            0.998370  \n"
     ]
    }
   ],
   "source": [
    "data_eval_ttt_2 = pyreadr.read_r(os.path.join(os.getcwd(), \"data_eval_ttt_2.rds\"))[None]\n",
    "print(data_eval_ttt_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce9ccfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fold  row_ids   truth response_glmnet  prob.suspect_glmnet  \\\n",
      "0     1        1  normal          normal             0.000327   \n",
      "1     1        3  normal          normal             0.040482   \n",
      "2     1        4  normal          normal             0.063884   \n",
      "3     1       23  normal          normal             0.083980   \n",
      "4     1       35  normal          normal             0.009852   \n",
      "\n",
      "   prob.normal_glmnet response_ranger  prob.suspect_ranger  prob.normal_ranger  \n",
      "0            0.999673          normal             0.000725            0.999275  \n",
      "1            0.959518          normal             0.466339            0.533661  \n",
      "2            0.936116          normal             0.177751            0.822249  \n",
      "3            0.916020          normal             0.087877            0.912123  \n",
      "4            0.990148          normal             0.011840            0.988160  \n"
     ]
    }
   ],
   "source": [
    "data_eval_ncv_2 = pyreadr.read_r(os.path.join(os.getcwd(), \"data_eval_ncv_2.rds\"))[None]\n",
    "print(data_eval_ncv_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4cb75934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>row_ids</th>\n",
       "      <th>truth</th>\n",
       "      <th>response_glmnet</th>\n",
       "      <th>prob.suspect_glmnet</th>\n",
       "      <th>prob.normal_glmnet</th>\n",
       "      <th>response_ranger</th>\n",
       "      <th>prob.suspect_ranger</th>\n",
       "      <th>prob.normal_ranger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>314</td>\n",
       "      <td>normal</td>\n",
       "      <td>suspect</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.025451</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.098596</td>\n",
       "      <td>0.901404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>315</td>\n",
       "      <td>normal</td>\n",
       "      <td>suspect</td>\n",
       "      <td>0.958650</td>\n",
       "      <td>0.041350</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.087827</td>\n",
       "      <td>0.912173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>316</td>\n",
       "      <td>normal</td>\n",
       "      <td>suspect</td>\n",
       "      <td>0.999728</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.063504</td>\n",
       "      <td>0.936496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>317</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.998229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>318</td>\n",
       "      <td>normal</td>\n",
       "      <td>suspect</td>\n",
       "      <td>0.999372</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.050645</td>\n",
       "      <td>0.949355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold  row_ids   truth response_glmnet  prob.suspect_glmnet  \\\n",
       "0     1      314  normal         suspect             0.974549   \n",
       "1     1      315  normal         suspect             0.958650   \n",
       "2     1      316  normal         suspect             0.999728   \n",
       "3     1      317  normal          normal             0.000050   \n",
       "4     1      318  normal         suspect             0.999372   \n",
       "\n",
       "   prob.normal_glmnet response_ranger  prob.suspect_ranger  prob.normal_ranger  \n",
       "0            0.025451          normal             0.098596            0.901404  \n",
       "1            0.041350          normal             0.087827            0.912173  \n",
       "2            0.000272          normal             0.063504            0.936496  \n",
       "3            0.999950          normal             0.001771            0.998229  \n",
       "4            0.000628          normal             0.050645            0.949355  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_eval_tmp_2 = pyreadr.read_r(os.path.join(os.getcwd(), \"data_eval_tmp_2.rds\"))[None]\n",
    "data_eval_tmp_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67563bd1",
   "metadata": {},
   "source": [
    "This leaves us with 3 evaluation scenarios:\n",
    "\n",
    "- ttt: models tuned via simple hold-out, the two respective winners (glmnet vs. ranger) are evaluated on the test data (after refitting)\n",
    "- ncv: models tuned in inner loop of nested CV, the two respective winners (glmnet vs. ranger) are evaluated in the outer loop (after refitting, one model for each of the 5 folds)\n",
    "- tmp: for a sensitivity analysis, we use the respective best hyperparameters for the glmnet and random forest from the train-tune-test design and evaluate the \"winners\" again for the temporal split (train one year, evaluate next year)\n",
    "(note that for a really consistent evaluation an independent \"inner\" loop is missing here...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb79e0",
   "metadata": {},
   "source": [
    "## 2.6 (Hypothetical) comparison of evaluation designs\n",
    "\n",
    "For a real-world ML problem, we would decide for a single experimental design a priori. \n",
    "In this exercise, we will compare different evaluation approaches for learning purposes. \n",
    "\n",
    "### Task\n",
    "\n",
    "For each design mentioned in section 2.5, calculate test performance estimates, initially for the AUC. \n",
    "(the function `roc_auc_score` that we already used previously should be of some help here)\n",
    "Here, you should report the individual performances (ranger, glmnet) and, in addition, the difference\n",
    "of performances for comparison (ranger - glmnet).\n",
    "\n",
    "Note that for nested CV, you have to write a small custom loop or function that calculate the metric\n",
    "per fold and then average the results for an overall (expected) performance assessment.\n",
    "\n",
    "When you are done, repeat with other metrics (tasks, design, ...) and try to interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "43edfd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc_ranger</th>\n",
       "      <th>auc_glmnet</th>\n",
       "      <th>auc_delta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ttt</th>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.980711</td>\n",
       "      <td>0.005003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncv</th>\n",
       "      <td>0.990150</td>\n",
       "      <td>0.963510</td>\n",
       "      <td>0.026641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tmp</th>\n",
       "      <td>0.957494</td>\n",
       "      <td>0.930004</td>\n",
       "      <td>0.027490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      auc_ranger  auc_glmnet  auc_delta\n",
       "name                                   \n",
       "ttt     0.985714    0.980711   0.005003\n",
       "ncv     0.990150    0.963510   0.026641\n",
       "tmp     0.957494    0.930004   0.027490"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed\n",
    "\n",
    "def evaluate(data_eval):\n",
    "    random_forest_auc = roc_auc_score(data_eval['truth'], data_eval['prob.suspect_ranger'], labels = [\"normal\", \"suspect\"])\n",
    "    glmnet_auc = roc_auc_score(data_eval['truth'], data_eval['prob.suspect_glmnet'], labels = [\"normal\", \"suspect\"])\n",
    "    diff_auc = random_forest_auc - glmnet_auc\n",
    "    return pd.DataFrame({\n",
    "        \"auc_ranger\": [random_forest_auc],\n",
    "        \"auc_glmnet\": [glmnet_auc],\n",
    "        \"auc_delta\": [diff_auc]\n",
    "    })\n",
    "\n",
    "def evaluate_cv(data_eval, aggr = pd.DataFrame.mean):\n",
    "    data_eval_grouped = data_eval.groupby(\"fold\").apply(evaluate, include_groups=False)\n",
    "    return pd.DataFrame(aggr(data_eval_grouped, axis = 0)).T\n",
    "\n",
    "results_ignoring_folds = evaluate(data_eval_ttt_2).assign(name = \"ttt\")\n",
    "results_ignoring_folds = pd.concat([results_ignoring_folds, evaluate(data_eval_ncv_2).assign(name = \"ncv\"), evaluate(data_eval_tmp_2).assign(name = \"tmp\")], ignore_index=True)\n",
    "results_ignoring_folds = results_ignoring_folds.set_index(\"name\")\n",
    "results_ignoring_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "21647e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_results = evaluate(data_eval_ttt_2).assign(name = \"ttt\")\n",
    "valid_results = pd.concat([valid_results, evaluate_cv(data_eval_ncv_2).assign(name = \"ncv\")], ignore_index=True)\n",
    "valid_results = pd.concat([valid_results, evaluate_cv(data_eval_tmp_2).assign(name = \"tmp\")], ignore_index=True)\n",
    "valid_results = valid_results.set_index(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "86f6043b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc_ranger</th>\n",
       "      <th>auc_glmnet</th>\n",
       "      <th>auc_delta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ttt</th>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.980711</td>\n",
       "      <td>0.005003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncv</th>\n",
       "      <td>0.990057</td>\n",
       "      <td>0.963949</td>\n",
       "      <td>0.026108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tmp</th>\n",
       "      <td>0.945188</td>\n",
       "      <td>0.910174</td>\n",
       "      <td>0.035013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      auc_ranger  auc_glmnet  auc_delta\n",
       "name                                   \n",
       "ttt     0.985714    0.980711   0.005003\n",
       "ncv     0.990057    0.963949   0.026108\n",
       "tmp     0.945188    0.910174   0.035013"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldglm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731344a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, recall_score, precision_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from  scipy.stats import norm, bootstrap\n",
    "from statsmodels.stats.proportion import proportion_confint, confint_proportions_2indep\n",
    "import pyreadr\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9675c929",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "To be able to conduct statistical inference for performance metrics in R, via \n",
    "\n",
    "- approximate confidence intervals,\n",
    "- exact confidence intervals,\n",
    "- and to translate such methods to method comparison.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "Load the evaluation data for the experiments from the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e67b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval_ttt_2 = pyreadr.read_r(os.path.join(os.getcwd(), \"data_eval_ttt_2.rds\"))[None]\n",
    "data_eval_ncv_2 = pyreadr.read_r(os.path.join(os.getcwd(), \"data_eval_ncv_2.rds\"))[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d75b129",
   "metadata": {},
   "source": [
    "For the train-tune-test split, we derive a few binary variables which may be useful later. Investigate them, if needed.\n",
    "If not specified otherwise, we always work on the \"ttt\" dataset in the following exercises.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9319fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = data_eval_ttt_2[[\"truth\"]]\n",
    "actual_01 = (actual == \"suspect\").astype(int).to_numpy().flatten()\n",
    "\n",
    "pred_glmnet = data_eval_ttt_2[\"response_glmnet\"]\n",
    "pred_glmnet_01 = (pred_glmnet == \"suspect\").astype(int).to_numpy().flatten()\n",
    "correct_glmnet_01 = (pred_glmnet_01 == actual_01).astype(int)\n",
    "\n",
    "pred_ranger = data_eval_ttt_2[\"response_ranger\"]\n",
    "pred_ranger_01 = (pred_ranger == \"suspect\").astype(int).to_numpy().flatten()\n",
    "correct_ranger_01 = (pred_ranger_01 == actual_01).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403eb521",
   "metadata": {},
   "source": [
    "## 3.1 Approximate inference for single proportions\n",
    "\n",
    "### Task\n",
    "\n",
    "For the train-tune-test split, calculate a 95\\% (Wald) confidence interval (CI) based on the normal approximation for the classification accuracy of each model (ranger, glmnet). Don't use any packages for this purpose but rather implement the CI yourself.\n",
    "\n",
    "- In addition, calculate a 95\\% Wald interval for sensitivity, specificity of each model and the respective difference of metrics.\n",
    "- What would need to be changed if only 80\\% coverage was required? \n",
    "\n",
    "Hint: you need to calculate the required quantities (the estimated proportion, its standard error and the critical value) based on the vectors `actual_01`, `correct_glmnet_01` and `correct_ranger_01` to calculate\n",
    "\n",
    "$$\n",
    "CI = \\left(\\ \\hat{p} - c_\\alpha \\hat{se}(\\hat{p}),\\ \\hat{p} + c_\\alpha \\hat{se}(\\hat{p}) \\ \\right)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b13423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea208817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9107a",
   "metadata": {},
   "source": [
    "## 3.2 Approximate & exact inference for single proportions\n",
    "\n",
    "### Task \n",
    "\n",
    "Compare the approximate results from exercise 3.1 with different (exact) alternative methods (at least those from the lecture, i.e. \"wilson\", \"logit\", \"clopper-pearson\") to calculate confidence intervals for for a single proportions. Also, if you find an implementation, compare the approximate CIs (method = \"wald\") with your own CIS from exercise 3.1. The statsmodels `proportion_confint` function should be very helpful here. I (Tom) am not sure if the logit and wald methods are implemented in statsmodels however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d071dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence intervals for glmnet accuracy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20187b47",
   "metadata": {},
   "source": [
    "## 3.3 Exact inference for the difference of two proportions\n",
    "\n",
    "### Task \n",
    "\n",
    "Calculate an exact confidence interval with the \"newcombe\" method.\n",
    "\n",
    "I suggest using the statsmodels `confint_proportions_2indep` function\n",
    "\n",
    "Check the arguments from this function carefully and decide which of them need to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a9c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7939ebb7",
   "metadata": {},
   "source": [
    "## 3.4 Inference for AUC\n",
    "\n",
    "### Task\n",
    "\n",
    "In this task, we want to study the ROC curve, which (to my knowledge) there isn't a simple function for in python that includes all functionality. Instead, we have to use different functions for different tasks.\n",
    "\n",
    "- fit a ROC curve for ranger and glmnet respectively; the sklearn `roc_curve` function should serve nicely\n",
    "- plot both ROC curves\n",
    "- estimate the associated AUCs and calculate a 95\\% CI for each one\n",
    "- estimate the difference in AUCs and calculate a 95\\% CI as well\n",
    "\n",
    "Hints: \n",
    "- The AUC can be computed with the `roc_auc_score`function we used previously\n",
    "- in the R exercise , the CIs are computed with the \"delong\" (normal approximation) and \"bootstrap\" methods. To my knowledge, the delong method isn't implemented in any python package. If you do find an implementation, please feel free to use it (and tell me :D) .\n",
    "  Otherwise, the CIs can be calculated with the bootstrap method. For example through the scipy `bootstrap` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16738196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fbc4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8ca07",
   "metadata": {},
   "source": [
    "## 3.5 Bootstrap inference for arbitrary metrics\n",
    "\n",
    "### Task\n",
    "\n",
    "Utilize the scipy `bootstrap` function to calculate confidence intervals for arbitrary metrics, e.g. accuracy, sensitivity, specificity, balanced accuracy... You may restrict your attention to the ranger model.\n",
    "\n",
    "For accuracy, you can use the `accuracy_score` function. The other metrics mentioned above arent' implemented in the libraries we've used so far. You can either find a library where they are implemented, or (which I would suggest) write a quick lambda function (`lambda x, y: ....`) that computes them. For this, you might want to use `precision_score` and `recall_score`, which are similar but different scores often used in ML that might only be a re-labeling away from being the right metric. The `pos_label` parameter of these functions might be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39670f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this codebox for your solution or add additional ones if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c007fc",
   "metadata": {},
   "source": [
    "## 3.6 Adjusting for multiple comparisons\n",
    "\n",
    "### Task (no code required)\n",
    "\n",
    "How could you adjust for multiple comparisons if you wanted to do simultaneous inference on all four metrics from the last exercise? Would that be a sensible idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d1704",
   "metadata": {},
   "source": [
    "## 3.7 Bootstrap inference after nested CV\n",
    "\n",
    "Execute the following chunks and inspect/interpret the results, there is no need to modify any code.\n",
    "\n",
    "The first chunk illustrates how data can be resampled in a \"hierarchical\" manner, respecting that we have observations within different folds in our evaluation data. Note that this is only a single bootstrap resample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5d64017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fold  row_ids    truth response_glmnet  prob.suspect_glmnet  \\\n",
      "0     1     1570   normal          normal             0.000597   \n",
      "1     1     1906  suspect         suspect             0.999587   \n",
      "2     1     1705  suspect         suspect             0.998765   \n",
      "3     1      431   normal          normal             0.106734   \n",
      "4     1     1637   normal          normal             0.000033   \n",
      "\n",
      "   prob.normal_glmnet response_ranger  prob.suspect_ranger  prob.normal_ranger  \n",
      "0            0.999403          normal             0.000000            1.000000  \n",
      "1            0.000413         suspect             0.996366            0.003634  \n",
      "2            0.001235         suspect             0.983038            0.016962  \n",
      "3            0.893266          normal             0.129006            0.870994  \n",
      "4            0.999967          normal             0.000000            1.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\splittgerber\\AppData\\Local\\Temp\\ipykernel_5748\\2983165629.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  resample = data_eval_ncv_2.groupby('fold').apply(lambda x: x.sample(n=n_obs_per_fold, replace=True)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "n_fold = data_eval_ncv_2['fold'].nunique()\n",
    "n_obs_per_fold = round(len(data_eval_ncv_2) / n_fold)\n",
    "\n",
    "resample = data_eval_ncv_2.groupby('fold').apply(lambda x: x.sample(n=n_obs_per_fold, replace=True)).reset_index(drop=True)\n",
    "print(resample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b1ac5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To perform the entire (naive and nested) bootstrap resampling, requires actually quite a bit of effort/code. If you are interested in the details, please check the R script \"scripts/3_nested_cv_bootstrap.R\". Here, we will just load the resampled data.\n",
    "\n",
    "```{r}\n",
    "resampled_cv_simple <- readRDS(\"data/resampled_auc_delta_cv_simple_ncv_2.rds\")\n",
    "\n",
    "resampled_cv_nested <- readRDS(\"data/resampled_auc_delta_cv_nested_ncv_2.rds\")\n",
    "```\n",
    "\n",
    "Preview bootstrap distribution:\n",
    "\n",
    "```{r}\n",
    "head(resampled_cv_simple$delta)\n",
    "head(resampled_cv_nested$delta)\n",
    "```\n",
    "\n",
    "Mean of bootstrap distribution:\n",
    "\n",
    "```{r}\n",
    "mean(resampled_cv_simple$delta)\n",
    "mean(resampled_cv_nested$delta)\n",
    "```\n",
    "\n",
    "\n",
    "Calculate percentile confidence intervals:\n",
    "\n",
    "```{r}\n",
    "quantile(resampled_cv_simple$delta, c(0.025, 0.975))\n",
    "quantile(resampled_cv_nested$delta, c(0.025, 0.975))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e237f5",
   "metadata": {},
   "source": [
    "## 3.8 Continue with your own evaluation study (open-ended)\n",
    "\n",
    "Congratulations, you made it through the prepared tasks.\n",
    "\n",
    "You can use the remaining time of the exercise to\n",
    "\n",
    "- specify appropriate metrics for your (upcoming) ML task,\n",
    "- think about appropriate methods for data splitting,\n",
    "- plan (or conduct) a sensible statistical analysis of your evaluation data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
